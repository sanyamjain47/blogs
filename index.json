[{"content":"Setup Install docker through the official website. Check if docker is installed by running docker --version in the terminal. Configure docker settings for memory and CPU usage. Run docker run hello-world to check if docker is working properly. Some starter containers Sometimes, it is not always possible to install stuff locally on your system. Like specific databases. In this case, you can use docker to run a container that has the database installed. You can then connect to this container from your local system.\nPostgres Run docker run --name postgres -e POSTGRES_PASSWORD=postgres -d -p 5432:5432 postgres This will run a postgres container with the name postgres and password postgres on port 5432. Other options: -e POSTGRES_USER=postgres to set the username to postgres. -e POSTGRES_DB=postgres to set the database name to postgres. -v /path/to/local/folder:/var/lib/postgresql/data to mount a local folder to the container. This is useful if you want to persist the data in the container. -d to run the container in the background. -p 5432:5432 to map the port 5432 of the container to the port 5432 of the host machine. MongoDB Run docker run --name some-mongo -d mongo This will run a mongo container with the name some-mongo. Other options: -e MONGO_INITDB_ROOT_USERNAME=mongoadmin to set the username to mongoadmin. -e MONGO_INITDB_ROOT_PASSWORD=secret to set the password to secret. -v /path/to/local/folder:/data/db to mount a local folder to the container. This is useful if you want to persist the data in the container. -d to run the container in the background. -p 27017:27017 to map the port 27017 of the container to the port 27017 of the host machine. Other useful commands docker ps to list all running containers. docker ps -a to list all containers. docker stop \u0026lt;container_name\u0026gt; to stop a container. docker rm \u0026lt;container_name\u0026gt; to remove a container. docker images to list all images. docker rmi \u0026lt;image_name\u0026gt; to remove an image. docker exec -it \u0026lt;container_name\u0026gt; bash to run a bash shell inside a container. docker logs \u0026lt;container_name\u0026gt; to view the logs of a container. docker inspect \u0026lt;container_name\u0026gt; to view the details of a container. docker inspect \u0026lt;container_name\u0026gt; | grep IPAddress to view the IP address of a container. Note Any data that is stored inside a container is lost when the container is removed. To persist data, you can mount a local folder to the container. Containers are not meant to be used as VMs. They are meant to be used as a single process. If you want to run multiple processes, you should use multiple containers. Containers are not meant to be used as a permanent storage solution. If you want to store data permanently, you should use a database. ","permalink":"https://sanyamjain47.github.io/blogs/posts/learn-docker/basic-commands/","summary":"Basic introduction to Docker.","title":"Docker Basic Commands"},{"content":"Coming Soon!\n","permalink":"https://sanyamjain47.github.io/blogs/posts/learn-docker/docker-compose/","summary":"What is Docker Compose and how to use it.","title":"Docker Compose"},{"content":"Why I want to learn Docker Motivation for containerization Instead of running a bunch of different scripts to install software, you can just run a single command to set up a container that already has the software you need. To deploy an application, you can just use the same \u0026ldquo;image\u0026rdquo; that you used to develop it. You can run multiple containers on the same machine, and they won\u0026rsquo;t interfere with each other. You can easily share images with other developers, so they can use the same software as you. What is a container? Their website says that \u0026ldquo;A Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings\u0026rdquo; A container is a running instance of an image. What is OCI? Open Container Initiative (OCI) is an open governance structure. Early days, multiple players were doing the same thing in a slightly different way. They all came together and formed OCI to standardize the way containers are built and run. Runtime Specification, Image Specification, and Distribution Specification are the three specifications that OCI has defined. Docker is a member of OCI. Difference between Desktop Container Platforms and Container Runtime Will come back to this later. I also don\u0026rsquo;t know the difference right now. IDK. What are orchestration tools? Orchestration tools are used to manage multiple containers. I only know this much about orchestration tools right now. Will come back to this later. IDK. Linux Building blocks Namespaces Namespaces are used to isolate resources. It basically wraps a global system resource in an abstraction that makes it appear to the processes within the namespace that they have their own isolated instance of the global resource. For example, a process in a namespace can have the same file name as a process in another namespace, but they will be different files. Control Groups (cgroups) Control Groups are used to limit the amount of resources that a process can use. For example, you can limit the amount of CPU, memory, and disk space that a process can use. Union File Systems Union File Systems are used to create layers of files. Allows files and directories of separate file systems, known as branches, to be transparently overlaid, forming a single coherent file system. For example, you can have a base layer that contains the operating system, and then you can have another layer that contains your application.= ","permalink":"https://sanyamjain47.github.io/blogs/posts/learn-docker/introduction/","summary":"Basic introduction to Docker.","title":"Docker Intro"},{"content":"Types of Networks in Docker The default bridge network Docker installs a virtual bridge, docker0, on the host machine during installation. This bridge acts as a virtual switch to which virtual network interfaces (veths) of containers are connected. Each container gets its own IP address in the Docker host\u0026rsquo;s private network range. When a container is created, Docker also creates a pair of virtual Ethernet (vEth) interfaces. One end of this pair is attached to the container, acting as its network interface. The other end is attached to the Docker bridge, allowing the container to communicate with the bridge and through it, with other containers or external networks. By default, containers are isolated from the outside world. To allow external access to services running in a container, specific ports must be exposed. This is done using the -p or --publish flag in the docker run command, which maps a port on the host to a port in the container. Docker manipulates iptables rules to forward traffic from the exposed host port to the corresponding container port. Containers connected to the default bridge network can communicate with each other using their internal IP addresses. This is possible because they are all connected to the docker0 bridge. The default bridge network is not recommended for complex applications as it lacks advanced features like automatic DNS resolution for container names. User-defined bridge networks User-defined bridges are created using the docker network create command. This command allows for additional configurations and customizations.\nThey are managed independently of the default bridge, offering more flexibility and control.\nOne of the significant advantages of user-defined bridges is automatic DNS resolution. Containers on the same user-defined bridge network can communicate with each other by their container names, which the network\u0026rsquo;s internal DNS server resolves.\nUser-defined bridges provide better network isolation compared to the default bridge. Each user-defined bridge network operates independently, creating a separate network environment for the containers connected to it.\nContainers attached to the same user-defined bridge network can talk to each other directly. By default, inter-container communication is enabled, but it can be disabled when creating the network for added security.\nContainers can be connected to a user-defined bridge network using the --network option when they are run or created.\nSimilar to the default bridge, ports can be exposed and mapped to the host in user-defined bridge networks using the -p or --publish option.\nUser-defined bridges allow for additional network configurations like specifying the subnet, the IP address range, the gateway, and even the IP address of individual containers.\nUser-defined bridge networks can offer improved performance in terms of network communication between containers compared to the default bridge network.\nThey are particularly useful in scenarios where multiple containers need to communicate on the same Docker host, but isolation from other containers or networks is also required.\nIdeal for development and testing environments where you need a quick and easy way to segment network traffic.\nThe Host network Containers using the host network type bypass Docker\u0026rsquo;s networking stack entirely. They share the network namespace with the Docker host, meaning they use the host\u0026rsquo;s IP address and have access to its network ports directly. This setup can offer improved network performance since it removes the network isolation between the container and the Docker host, reducing the overhead associated with network virtualization. In host networking mode, port mapping is not required for containers to communicate with the outside world, as they are using the host\u0026rsquo;s network directly. Containers can open any port directly on the host\u0026rsquo;s network interface. Since containers share the host\u0026rsquo;s network namespace, they also use the host\u0026rsquo;s DNS settings and routing tables. This simplifies scenarios where specific network configurations on the host need to be reused by the containers. The lack of network isolation in host mode can be a security concern. Containers have the same network privileges as processes running directly on the host. This setup is generally not recommended for multi-tenant environments or where container isolation is a priority. Host networking is often used in situations where high network throughput is required and security or network isolation is not a major concern. It\u0026rsquo;s commonly used for network debugging purposes or for running services that need to manage the host’s network stack directly. Containers in host network mode cannot be connected to other Docker networks. They only have access to the host\u0026rsquo;s network. Unlike bridge networks, host networking provides no network isolation, no automatic DNS resolution for other containers, and does not use internal Docker IP addresses. The MACVLAN network The macvlan network type in Docker is designed to enable containers to appear as physical devices on your network. This setup is particularly useful in scenarios where you need containers to have full network stack independence from the host.\nMacvlan networks allow containers to have their own MAC address and IP address on the physical network, making them appear as physical network devices to other devices on the same LAN.\nThis approach bypasses the need for port mapping, as each container can be directly addressed on the network.\nA macvlan network is created as a Docker network type.\nEach container connected to a macvlan network gets its own unique MAC address.\nThe Docker host\u0026rsquo;s network interface is used as the parent interface for the macvlan network.\nContainers on a macvlan network are isolated at Layer 2 (Data Link Layer), providing a high degree of network isolation and security.\nIt\u0026rsquo;s particularly useful in enterprise environments where containers need to be integrated into existing VLANs or need to be compliant with network policies that require devices to have their own MAC/IP.\nIdeal for running legacy applications that expect to be on a physical network.\nA subinterface in macvlan is essentially a subdivision of a physical network interface.\nIt allows for the creation of multiple distinct logical interfaces, each with its own MAC and IP address, on a single physical interface.\nThis setup can be used to create different macvlan networks segregated at Layer 2, which can be useful for creating VLAN-like behavior within Docker.\nMacvlan provides strong network separation and can offer better performance for certain network-intensive applications.\nHowever, it requires careful planning in terms of network security and management, as each container is effectively a network endpoint.\nMacvlan does not allow for container-to-host network communication using the macvlan interface. This is a key difference from other network types.\nIt requires a compatible physical network environment and may not be suitable for all deployments, particularly where network hardware or policies do not support this kind of configuration.\nThe IPVLAN network The ipvlan network type in Docker provides a way to assign multiple IP addresses to a single network interface, allowing containers to share a common physical network interface while maintaining separate network identities. This approach is similar to macvlan but with some key differences. Here\u0026rsquo;s an overview of ipvlan and its modes:\nIPVLAN Network Overview: Designed to give containers their own IP addresses and to allow them to appear as unique devices on the network, similar to macvlan. Useful in environments where macvlan might not be suitable due to MAC address limitations. Containers share the host\u0026rsquo;s physical network interface but have independent IP addresses. This design makes ipvlan more efficient in environments where MAC address limitation is a concern. Layer 2 (L2) Mode:\nIn L2 mode, ipvlan acts similarly to a traditional Ethernet network. Containers are assigned their own IP addresses but share the MAC address of the host\u0026rsquo;s network interface. Containers can communicate with each other and with external devices on the same LAN. Good for environments where layer 3 (routing) capabilities are not necessary or desired. Layer 3 (L3) Mode:\nIn L3 mode, ipvlan operates at the network layer. Each container gets its own IP address, and the network traffic is routed. Containers are not able to communicate with each other at layer 2; instead, all inter-container communication is routed. Ideal for situations where network segmentation and routing are more important than broadcast or multicast traffic. Key Features: Ipvlan is more efficient in terms of the number of MAC addresses used, making it suitable for large-scale deployments.\nWorks with existing network infrastructures that support VLANs and traditional routing.\nProvides network isolation at either the data link layer (L2) or network layer (L3), depending on the mode used.\nUseful in environments with a large number of containers where MAC address exhaustion could be an issue.\nSuitable for scenarios requiring advanced network segmentation and policy enforcement.\nSimilar to macvlan, ipvlan in both modes does not allow containers to communicate with the host over the ipvlan interface.\nRequires a good understanding of network fundamentals to deploy effectively.\nNotes I know this has been a lot of notes but this is to keep a track of what I have learned. I will be using this as a reference for future projects. I will also be adding more notes as I learn more about docker. Below is a comparison of similar network types.\n1. Macvlan vs. Ipvlan: MAC Address Handling:\nMacvlan: Assigns a unique MAC address to each container, making each container appear as a separate physical device on the network. Ipvlan: Containers share the MAC address of the host\u0026rsquo;s network interface. This approach reduces the number of MAC addresses used in the network. Network Layer Operation:\nMacvlan: Operates at Layer 2 (Data Link Layer), allowing containers to communicate like physical devices connected to the same LAN. Ipvlan: Can operate either at Layer 2 (L2 mode) like Macvlan or at Layer 3 (L3 mode), focusing on network segmentation and routing. Host Communication:\nBoth Macvlan and Ipvlan: Containers cannot communicate with the host over the same interface. Use Case:\nMacvlan: Suitable for environments where containers need to be treated as distinct physical devices, often used in enterprise settings. Ipvlan: Preferred in large-scale deployments to avoid MAC address exhaustion and when network layer segmentation is crucial. 2. Macvlan/Ipvlan vs. Bridge: Network Isolation:\nMacvlan/Ipvlan: Containers are isolated at the network level, appearing as unique network entities. Bridge: Containers on the same bridge network can communicate with each other but are isolated from the host network. IP and MAC Address:\nMacvlan/Ipvlan: Containers have their own IP and (in Macvlan) MAC addresses, separate from the host. Bridge: Containers use internal Docker IP addresses; the host manages network traffic through NAT and port mapping. Performance:\nMacvlan/Ipvlan: Potentially higher network performance due to direct access to the physical network. Bridge: Slightly lower performance due to NAT overhead. Ease of Use:\nMacvlan/Ipvlan: Requires more network knowledge and careful planning. Bridge: Easier to set up and use, especially for those new to Docker networking. Port Mapping:\nMacvlan/Ipvlan: No need for port mapping, as each container can directly expose ports. Bridge: Requires port mapping to expose container services to the external network. Summary: Macvlan is ideal for environments where containers need to appear as physical devices on the network. Ipvlan is a more efficient alternative to Macvlan, especially in large-scale deployments or when layer 3 routing is needed. Bridge networks are the simplest to use, suitable for most standard Docker deployments where complex networking is not a requirement. ","permalink":"https://sanyamjain47.github.io/blogs/posts/learn-docker/docker-network/","summary":"Different types of Networks in Docker","title":"Docker Networks"},{"content":"Dockerfiles Dockerfiles are used to build images. They are a list of instructions that are used to build an image. docker build -t \u0026lt;image_name\u0026gt; \u0026lt;path_to_dockerfile\u0026gt; is used to build an image from a Dockerfile. Example This example is based on https://github.com/sidpalas/devops-directive-docker-course/tree/main/05-example-web-application . This is part of the course that I am using to learn Docker.\napi-node FROM ubuntu RUN apt update RUN apt install nodejs -y In the above example, we are using the ubuntu image as the base image. Currently, we are not specifying the version of the image. This is not recommended. We should always specify the version of the image. Also, every command in the Dockerfile creates a new layer. This is not recommended. We should always try to combine multiple commands into a single command. We are using the RUN command to run the commands. Now, let\u0026rsquo;s merge the two RUN commands into a single command. Copy the files also now. We also added npm since we forgot to add it earlier. FROM ubuntu RUN apt update \u0026amp;\u0026amp; apt install nodejs npm -y COPY . . RUN npm install Now, add the CMD command to run the application. FROM ubuntu RUN apt update \u0026amp;\u0026amp; apt install nodejs npm -y COPY . . RUN npm install CMD [\u0026#34;npm\u0026#34;,\u0026#34;run\u0026#34;,\u0026#34;dev\u0026#34;] Now let\u0026rsquo;s start optimizing the Dockerfile. For starters, since we are only using this image for nodejs, we can use the node image as the base image instead of the ubuntu image. Also, we should specify the version of the image. Let\u0026rsquo;s use node:19.6-alpine as the image. Instead of copying the whole source directory, we can copy the package.json file first and then run npm install. This will allow us to cache the node_modules folder. This will speed up the build process. We can also use WORKDIR to set the working directory to /app. This will allow us to run the npm install command without specifying the path. When using the copy command we can have some files that we don\u0026rsquo;t want to copy. We can use .dockerignore to specify the files that we don\u0026rsquo;t want to copy. Or we can use the COPY command to copy only the files that we want to copy. We now change npm run dev to node index.js. All the changes are shown below. FROM node:19.6-alpine WORKDIR /usr/src/app COPY package*.json ./ RUN npm install COPY ./src . CMD [\u0026#34;node\u0026#34;,\u0026#34;index.js\u0026#34;] Up until now, we have been running the commands as a root user. This is not recommended. We should always run the commands as a non-root user. We can use the USER command to specify the user that we want to run the commands as. This involves creating a user and then switching to that user. Also, while copying the files, we need to make sure that the user has the required permissions to copy the files. We can use the chown command to change the owner of the files. FROM node:19.6-alpine WORKDIR /usr/src/app COPY package*.json ./ RUN npm install USER node COPY --chown=node:node ./src . CMD [\u0026#34;node\u0026#34;,\u0026#34;index.js\u0026#34;] \u0026ndash; Now, We can do some node.js specific optimizations.\nWe can start by using an ENV variable to specify the environment. This will allow us to use different configurations for different environments. We can also use npm ci instead of npm install. This will install the exact versions of the dependencies specified in the package-lock.json file. This will speed up the build process. Adding --only=production will install only the production dependencies. This will reduce the size of the image. Also, we would like to EXPOSE the port 3000 so that we can access the application from outside the container. FROM node:19.6-alpine WORKDIR /usr/src/app ENV NODE_ENV production COPY package*.json ./ RUN npm ci --only=production USER node COPY --chown=node:node ./src . EXPOSE 3000 CMD [\u0026#34;node\u0026#34;,\u0026#34;index.js\u0026#34;] We can change npm ci --only=production to --mount=type=cache,target=/usr/src/app/.npm \\ npm set cache /usr/src/app/.npm \u0026amp;\u0026amp; \\ npm ci --only=production to use a cache volume. This will speed up the build process even more.\ngo-app We start with the golang image. We set the Workdir. We go with a naive approach and then optimize it. from golang WORKDIR /app COPY . . RUN go mod download CMD [\u0026#34;go\u0026#34;,\u0026#34;run\u0026#34;,\u0026#34;./main.go\u0026#34;] Start by adding a version to the image. We use golang:1.19-alpine as the image. Also, when using go, we don\u0026rsquo;t want to use go run main.go as our command. We want to build the binary and then run the binary. Also, we want to download the dependencies first and then copy the source code. This will allow us to cache the dependencies. This will speed up the build process. FROM golang:1.19-alpine WORKDIR /app COPY go.mod go.sum ./ RUN go mod download COPY . . RUN go build -o api-golang CMD [\u0026#34;./api-golang\u0026#34;] For this image, we can do 1 more thing. We can use a multi-stage build. This will allow us to build the binary in one stage and then copy the binary to the final image. This will reduce the size of the image. There is an image called scratch that is used to create images from scratch. This is useful when we want to create images that don\u0026rsquo;t have any dependencies. We can use this image as the base image for our final image. FROM golang:1.19-buster AS build WORKDIR /app COPY go.mod go.sum ./ RUN go mod download COPY . . RUN go build \\ -ldflags=\u0026#34;-linkmode external -extldflags -static\u0026#34; \\ -tags netgo \\ -o api-golang FROM scratch COPY --from=build /app/api-golang /app/api-golang CMD [\u0026#34;/app/api-golang\u0026#34;] We can also use ENV to specify the environment. In gin the package used in golang to create the api, we can use ENV GIN_MODE=release to specify the environment. We can also use EXPOSE to expose the port 8080 so that we can access the application from outside the container. Also, we can use a cache mount similarly to how we did it in the api-node image. Note, we have also changed the image from alpine to buster. This is because alpine doesn\u0026rsquo;t have glibc installed. This causes issues when running the binary. So, we use buster instead. Since, we are using the buster image as an intermediate image, this doesn\u0026rsquo;t affect the size of the final image. FROM golang:1.19-buster AS build WORKDIR /app COPY go.mod go.sum ./ RUN --mount=type=cache,target=/go/pkg/mod \\ --mount=type=cache,target=/root/.cache/go-build \\ go mod download COPY . . RUN go build \\ -ldflags=\u0026#34;-linkmode external -extldflags -static\u0026#34; \\ -tags netgo \\ -o api-golang FROM scratch ENV GIN_MODE release COPY --from=build /app/api-golang /app/api-golang EXPOSE 8080 CMD [\u0026#34;/app/api-golang\u0026#34;] Last thing we can do is to use a non-root user. This is similar to what we did in the api-node image. FROM golang:1.19-buster AS build WORKDIR /app RUN useradd -u 10001 appuser COPY go.mod go.sum ./ RUN --mount=type=cache,target=/go/pkg/mod \\ --mount=type=cache,target=/root/.cache/go-build \\ go mod download COPY . . RUN go build \\ -ldflags=\u0026#34;-linkmode external -extldflags -static\u0026#34; \\ -tags netgo \\ -o api-golang FROM scratch ENV GIN_MODE release COPY --from=build /etc/passwd /etc/passwd COPY --from=build /app/api-golang /app/api-golang USER appuser EXPOSE 8080 CMD [\u0026#34;/app/api-golang\u0026#34;] react-app We start with the node image. This would be very similar to the api-node image. Not many optimizations to do here. We learnt alot about optimizing the api-node image. We can use the same optimizations here. We can also use a multi-stage build to reduce the size of the image. FROM node:19.4-bullseye as build WORKDIR /user/src/app COPY package*.json ./ RUN --mount=type=cache,target=/usr/src/app/.npm \\ npm set cache /usr/src/app/.npm \u0026amp;\u0026amp; \\ npm ci COPY . . RUN npm run build ### FROM nginxinc/nginx-unprivileged:1.23-alpine-perl COPY nginx.conf /etc/nginx/conf.d/default.conf COPY --from=build /usr/src/app/dist /usr/share/nginx/html EXPOSE 8080 General Principles while writing Dockerfiles Pin Speicific Versions Base images System Dependencies Application Dependencies Use small + secure base images Protect the layer cache Use mutli-stage builds Order commands by frequency of change Copy dependency requirements first -\u0026gt; Intall dependencies -\u0026gt; Copy source code Use Cache mounts Use COPY \u0026ndash;link Be explicit Set working directory Set environment variables Set standard port Avoid unnecessary packages COPY specific files Use .dockerignore Use non-root user Install only production dependencies Notes COPY --link is very useful in building an image as this makes the copy step an independent step. This means that if the source code changes, the cache for the dependencies will not be invalidated. This will speed up the build process.\n","permalink":"https://sanyamjain47.github.io/blogs/posts/learn-docker/docker-file/","summary":"What are Dockerfiles and how to use them.","title":"Dockerfiles"},{"content":"Data in Docker Containers By default, any data that is stored inside a container is lost when the container is removed. We should never make modifications to the container\u0026rsquo;s file system. Any modification done to get it to the state we want, should be done in the image itself. To persist data, you can mount a volume. There are two types of volumes: Bind Mounts: Mounts a local folder to the container. Volumes: Creates a volume that is managed by Docker Desktop VM. Bind Mounts Bind mounts are used to mount a local folder to the container. Example Run docker run -it --rm --mount type=bind,source=\u0026quot;$(pwd)\u0026quot;/my-data,destination=/my-data/ ubuntu:22.04 to mount the current directory to the container. Volume Mounts Volume mounts are used to create a volume that is managed by Docker Desktop VM. Docker recommends using volume mounts over bind mounts. Volumes are stored in /var/lib/docker/volumes on Linux and C:\\ProgramData\\Docker\\volumes on Windows. Example Run docker volume create my-volume to create a volume. Run docker run -it --rm --mount source=my-volume,destination=/my-data/ ubuntu:22.04 to attach the volume Run docker volume inspect my-volume to inspect the volume. ","permalink":"https://sanyamjain47.github.io/blogs/posts/learn-docker/volumes-in-docker/","summary":"How to use volumes in Docker.","title":"Volumes in Docker"}]